{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5054e-e1fb-4b3b-b7ab-8d6f8e69e5ac",
   "metadata": {},
   "source": [
    "## Connection to Cohere\n",
    "https://python.langchain.com/docs/integrations/chat/cohere\n",
    "\n",
    "\n",
    "The Cohere API offers a free tier for testing purposes. However, the exact number of tokens that can be used with the free version depends on the specific tier and usage limits.\\n\\nFor the Free Tier: Trial plan, you have a limited number of tokens that you can use on a monthly basis. The exact number of tokens varies according to your usage behavior and can be viewed in the Cohere API dashboard. \\n\\nAdditionally, the Free Tier: Trial plan has other usage limits such as the maximum number of requests per minute, the maximum number of documents in a batch, and the maximum document size.\\n\\nIt's important to note that the Free Tier is intended for testing and experimentation, and may not be suitable for production-level usage. If you require more tokens or higher usage limits, you can upgrade to one of Cohere's paid plans, which offer more flexibility and higher limits. \\n\\nI hope this answers your question. If you would like more information on pricing and plans, please visit the Cohere website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a30c3-f568-4357-8708-4a26ee6107c2",
   "metadata": {},
   "source": [
    "## Cohere embeddings\n",
    "https://python.langchain.com/docs/integrations/text_embedding/cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c2610-4c33-470a-a8d6-be5b4c8c6d28",
   "metadata": {},
   "source": [
    "## ChromaDB\n",
    "\n",
    "Supported embeddings functions\n",
    "https://docs.trychroma.com/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38447db-05a1-4f03-b430-f98759ee0c91",
   "metadata": {},
   "source": [
    "---\n",
    "Settings for chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0ea45a-45e9-461c-9ca1-b8dccd14e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "# This allows us to create a client that connects to the server\n",
    "collection = chroma_client.create_collection(name=\"my_collection_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6a23c-87d2-42e6-9bc1-dd434f3286b9",
   "metadata": {},
   "source": [
    "In Chroma, the chroma_client.create_collection/get_or_create_collection method allows us to create a collection of embedding vectors. collection is like a table in the relational database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae6fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7b69f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COHERE_api_key: str = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137cbc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cohere\n",
      "  Downloading cohere-5.5.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\mavix\\appdata\\roaming\\python\\python39\\site-packages (from cohere) (1.34.127)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Downloading fastavro-1.9.4-cp39-cp39-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from cohere) (0.27.0)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from cohere)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from cohere) (2.7.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from cohere) (0.19.1)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from cohere) (4.12.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.127 in c:\\users\\mavix\\appdata\\roaming\\python\\python39\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (1.34.127)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\mavix\\appdata\\roaming\\python\\python39\\site-packages (from boto3<2.0.0,>=1.34.0->cohere) (0.10.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpx>=0.21.2->cohere) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpx>=0.21.2->cohere) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from pydantic>=1.9.2->cohere) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\mavix\\appdata\\roaming\\python\\python39\\site-packages (from tokenizers<1,>=0.15->cohere) (0.23.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from botocore<1.35.0,>=1.34.127->boto3<2.0.0,>=1.34.0->cohere) (2.9.0)\n",
      "INFO: pip is looking at multiple versions of botocore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting botocore<1.35.0,>=1.34.127 (from boto3<2.0.0,>=1.34.0->cohere)\n",
      "  Downloading botocore-1.34.133-py3-none-any.whl.metadata (5.7 kB)\n",
      "  Downloading botocore-1.34.132-py3-none-any.whl.metadata (5.7 kB)\n",
      "  Downloading botocore-1.34.131-py3-none-any.whl.metadata (5.7 kB)\n",
      "  Downloading botocore-1.34.130-py3-none-any.whl.metadata (5.7 kB)\n",
      "  Downloading botocore-1.34.129-py3-none-any.whl.metadata (5.7 kB)\n",
      "  Downloading botocore-1.34.128-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: pip is still looking at multiple versions of botocore to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading types_requests-2.32.0.20240523-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.32.0.20240521-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240403-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240402-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240310-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240125-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.20231231-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.10-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.9-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading types_requests-2.31.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading types_requests-2.31.0.7-py3-none-any.whl.metadata (1.4 kB)\n",
      "  Downloading types_requests-2.31.0.6-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting types-urllib3 (from types-requests<3.0.0,>=2.0.0->cohere)\n",
      "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere)\n",
      "  Using cached urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from anyio->httpx>=0.21.2->cohere) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.127->boto3<2.0.0,>=1.34.0->cohere) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.4.6)\n",
      "Downloading cohere-5.5.8-py3-none-any.whl (173 kB)\n",
      "   ---------------------------------------- 0.0/173.8 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 61.4/173.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 173.8/173.8 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading fastavro-1.9.4-cp39-cp39-win_amd64.whl (546 kB)\n",
      "   ---------------------------------------- 0.0/546.3 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 235.5/546.3 kB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 501.8/546.3 kB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 546.3/546.3 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-urllib3, urllib3, types-requests, parameterized, jmespath, httpx-sse, fastavro, cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "Successfully installed cohere-5.5.8 fastavro-1.9.4 httpx-sse-0.4.0 jmespath-1.0.1 parameterized-0.9.0 types-requests-2.31.0.6 types-urllib3-1.26.25.14 urllib3-1.26.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2024.6.0 requires cloudpickle>=1.5.0, which is not installed.\n",
      "distributed 2024.6.0 requires locket>=1.0.0, which is not installed.\n",
      "distributed 2024.6.0 requires msgpack>=1.0.0, which is not installed.\n",
      "distributed 2024.6.0 requires sortedcontainers>=2.0.5, which is not installed.\n",
      "distributed 2024.6.0 requires tblib>=1.6.0, which is not installed.\n",
      "distributed 2024.6.0 requires toolz>=0.10.0, which is not installed.\n",
      "distributed 2024.6.0 requires zict>=3.0.0, which is not installed.\n",
      "evidently 0.4.27 requires dynaconf>=3.2.4, which is not installed.\n",
      "evidently 0.4.27 requires nltk>=3.6.7, which is not installed.\n",
      "evidently 0.4.27 requires pandas[parquet]>=1.3.5, which is not installed.\n",
      "evidently 0.4.27 requires plotly>=5.10.0, which is not installed.\n",
      "evidently 0.4.27 requires scikit-learn>=1.0.1, which is not installed.\n",
      "evidently 0.4.27 requires scipy>=1.10.0, which is not installed.\n",
      "evidently 0.4.27 requires statsmodels>=0.12.2, which is not installed.\n",
      "evidently 0.4.27 requires typing-inspect>=0.9.0, which is not installed.\n",
      "evidently 0.4.27 requires watchdog>=3, which is not installed.\n",
      "moto 4.2.14 requires cryptography>=3.3.1, which is not installed.\n",
      "moto 4.2.14 requires responses>=0.13.0, which is not installed.\n",
      "moto 4.2.14 requires werkzeug!=2.2.0,!=2.2.1,>=0.5, which is not installed.\n",
      "moto 4.2.14 requires xmltodict, which is not installed.\n",
      "pmdarima 2.0.4 requires Cython!=0.29.18,!=0.29.31,>=0.29, which is not installed.\n",
      "pmdarima 2.0.4 requires joblib>=0.11, which is not installed.\n",
      "pmdarima 2.0.4 requires pandas>=0.19, which is not installed.\n",
      "pmdarima 2.0.4 requires scikit-learn>=0.22, which is not installed.\n",
      "pmdarima 2.0.4 requires scipy>=1.3.2, which is not installed.\n",
      "pmdarima 2.0.4 requires statsmodels>=0.13.2, which is not installed.\n",
      "tbats 1.1.3 requires scikit-learn, which is not installed.\n",
      "tbats 1.1.3 requires scipy, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "# pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9915ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting langchain\n",
      "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.31-cp39-cp39-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.9.5-cp39-cp39-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.1.82-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain) (2.7.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain) (8.4.2)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl.metadata (32 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.0.3-cp39-cp39-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "Downloading aiohttp-3.9.5-cp39-cp39-win_amd64.whl (371 kB)\n",
      "   ---------------------------------------- 0.0/371.6 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/371.6 kB 660.6 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 92.2/371.6 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 256.0/371.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 371.6/371.6 kB 2.3 MB/s eta 0:00:00\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.82-py3-none-any.whl (127 kB)\n",
      "Using cached SQLAlchemy-2.0.31-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Using cached greenlet-3.0.3-cp39-cp39-win_amd64.whl (290 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Using cached yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Installing collected packages: multidict, jsonpointer, greenlet, frozenlist, attrs, async-timeout, yarl, SQLAlchemy, jsonpatch, aiosignal, langsmith, aiohttp, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.10 langchain-text-splitters-0.2.2 langsmith-0.1.82 multidict-6.0.5 yarl-1.9.4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "litestar 2.9.0 requires msgspec>=0.18.2, which is not installed.\n",
      "trio 0.25.1 requires cffi>=1.14; os_name == \"nt\" and implementation_name != \"pypy\", which is not installed.\n",
      "trio 0.25.1 requires outcome, which is not installed.\n",
      "trio 0.25.1 requires sortedcontainers, which is not installed.\n",
      "trio-websocket 0.11.1 requires wsproto>=0.14, which is not installed.\n",
      "visions 0.7.6 requires multimethod>=1.4, which is not installed.\n",
      "visions 0.7.6 requires networkx>=2.4, which is not installed.\n",
      "visions 0.7.6 requires pandas>=2.0.0, which is not installed.\n",
      "ydata-profiling 4.8.3 requires dacite>=1.8, which is not installed.\n",
      "ydata-profiling 4.8.3 requires htmlmin==0.1.12, which is not installed.\n",
      "ydata-profiling 4.8.3 requires multimethod<2,>=1.4, which is not installed.\n",
      "ydata-profiling 4.8.3 requires numba<1,>=0.56.0, which is not installed.\n",
      "ydata-profiling 4.8.3 requires pandas!=1.4.0,<3,>1.1, which is not installed.\n",
      "ydata-profiling 4.8.3 requires scipy<1.14,>=1.4.1, which is not installed.\n",
      "ydata-profiling 4.8.3 requires seaborn<0.14,>=0.10.1, which is not installed.\n",
      "ydata-profiling 4.8.3 requires statsmodels<1,>=0.13.2, which is not installed.\n",
      "ydata-profiling 4.8.3 requires typeguard<5,>=3, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "# pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f178640-cc68-428b-ae37-d0a24e33998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "COHERE_api_key: str = os.getenv(\"COHERE_API_KEY\")\n",
    "#openai_ef = embedding_functions.OpenAIEmbeddingFunction(model_name=\"text-embedding-ada-002\")\n",
    "cohere_ef  = embedding_functions.CohereEmbeddingFunction(api_key=COHERE_api_key,  model_name=\"large\")\n",
    "metadata_options = {\n",
    "    \"hnsw:space\": \"ip\"  # You can change this to \"ip\" or \"cosine\" if needed\n",
    "}\n",
    "\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"my_collection\", metadata=metadata_options, embedding_function=cohere_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9a8d6-bc92-4811-bfdf-c2353ee04a97",
   "metadata": {},
   "source": [
    "[similarity-metrics](https://medium.com/@junxie2/semantic-search-similarity-metrics-e215a0d65885)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cff0edc-c3aa-40da-bca9-ce134a4ce635",
   "metadata": {},
   "source": [
    "Now, let’s proceed to store data in our collection. To begin, we’ll define a text_splitter that assists in breaking down documents into smaller chunks. Following that, we’ll generate a unique UUID for each segment and then insert both the documents and their respective UUIDs into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9c9c2b-93c8-4623-827e-3345441bed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "content = 'file_content'\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"], chunk_size=200, chunk_overlap=30)\n",
    "\n",
    "docs = text_splitter.create_documents([content])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b4e380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='file_content')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b9934f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseModel.parse_raw of <class 'langchain_core.documents.base.Document'>>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52d671e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='F:\\\\Proyectos\\\\PiConsulting\\\\challenge_rag_llm\\\\docs\\\\documento.docx')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30151b7-bf6f-4410-a744-6c713992fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document for 5856e70e-3392-11ef-976a-4c77cb1e1a4e\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected each embedding in the embeddings to be a list, got ['tuple']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m uuid_name \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid1()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument for\u001b[39m\u001b[38;5;124m\"\u001b[39m, uuid_name)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muuid_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages\\chromadb\\api\\models\\Collection.py:80\u001b[0m, in \u001b[0;36mCollection.add\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     42\u001b[0m     ids: OneOrMany[ID],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     uris: Optional[OneOrMany[URI]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add embeddings to the data store.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m        ids: The ids of the embeddings you wish to add\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     (\n\u001b[0;32m     75\u001b[0m         ids,\n\u001b[0;32m     76\u001b[0m         embeddings,\n\u001b[0;32m     77\u001b[0m         metadatas,\n\u001b[0;32m     78\u001b[0m         documents,\n\u001b[0;32m     79\u001b[0m         uris,\n\u001b[1;32m---> 80\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_and_prepare_embedding_set\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muris\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_add(ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, embeddings, metadatas, documents, uris)\n",
      "File \u001b[1;32mc:\\Users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:269\u001b[0m, in \u001b[0;36mCollectionCommon._validate_and_prepare_embedding_set\u001b[1;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# At this point, we know that one of documents or images are provided from the validation above\u001b[39;00m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mimages)\n",
      "File \u001b[1;32mc:\\Users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages\\chromadb\\api\\models\\CollectionCommon.py:558\u001b[0m, in \u001b[0;36mCollectionCommon._embed\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must provide an embedding function to compute embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.trychroma.com/guides/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    557\u001b[0m     )\n\u001b[1;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages\\chromadb\\api\\types.py:198\u001b[0m, in \u001b[0;36mEmbeddingFunction.__init_subclass__.<locals>.__call__\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m: EmbeddingFunction[D], \u001b[38;5;28minput\u001b[39m: D) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Embeddings:\n\u001b[0;32m    197\u001b[0m     result \u001b[38;5;241m=\u001b[39m call(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_cast_one_to_many_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mavix\\.conda\\envs\\pi_challenge_rag_llm\\lib\\site-packages\\chromadb\\api\\types.py:492\u001b[0m, in \u001b[0;36mvalidate_embeddings\u001b[1;34m(embeddings)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected embeddings to be a list with at least one item, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    490\u001b[0m     )\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m embeddings]):\n\u001b[1;32m--> 492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    493\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected each embedding in the embeddings to be a list, got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([\u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39me\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39membeddings]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    495\u001b[0m     )\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(embeddings):\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(embedding) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected each embedding in the embeddings to be a list, got ['tuple']"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    uuid_name = uuid.uuid1()\n",
    "    print(\"document for\", uuid_name)\n",
    "    collection.add(ids=[str(uuid_name)], documents=doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb4dfb-565f-48ab-b4a3-91c087fa3525",
   "metadata": {},
   "source": [
    "Ultimately, our objective is to **retrieve the pertinent documents from the database**. To achieve this, we employ the collection.query option. During the querying process, we will provide the input text and specify the number of results we wish to retrieve from the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a5feca7-583b-4ecc-bffe-61c2f2741595",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'quien es Zara?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83b34bea-3ead-4cea-8b2a-b9d513b843e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = collection.query(query_texts=[question], n_results=1)['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f5f82-3b27-46d0-b92e-dbf345cadd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
