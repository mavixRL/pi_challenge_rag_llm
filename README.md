# Proyecto Challenge RAG con LLM

Introduction
------------
Este proyecto consiste en desarrollar una soluciÃ³n basada en la generaciÃ³n de respuestas aumentadas (RAG) mediante el uso de modelos de lenguaje de gran escala (LLMs). El enfoque principal es proporcionar una interacciÃ³n eficiente y precisa entre el usuario y el modelo, permitiendo obtener respuestas contextuales y relevantes a partir de documentos especÃ­ficos. La soluciÃ³n implicarÃ¡ la creaciÃ³n de una API que servirÃ¡ de puente entre el usuario y el LLM.

Desarrollado por Mavix Arias Aguila [https://www.linkedin.com/in/mavix-arias-aguila/]

## Contenido
1. [Estructura del Proyecto](#estructura-del-proyecto)
2. [Tecnologias](#tecnologias)
3. [ConfiguraciÃ³n del Entorno Local](#configuraciÃ³n-del-entorno-local)
    1. [Clonar el repositorio](#clonar-el-repositorio)
    2. [Crear entorno virtual](#crear-entorno-virtual)
    3. [Instalar Dependencias](#instalar-dependencias)
    4. [Inicializar AplicaciÃ³n](#inicializar-aplicaciÃ³n)
4. [DocumentaciÃ³n de la API](#documentaciÃ³n-de-la-api)
5. [ImplementaciÃ³n con Docker(Opcional)](#implementaciÃ³n-con-docker(opcional))

## Estructura del Proyecto

```
challenge_rag_llm/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                 # Contiene controladores/routers/schemas/logging
â”‚   â”‚   â”œâ”€â”€ endpoints/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ask_cohere.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ask_openai.py
â”‚   â”‚   â”‚   â””â”€â”€ root.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ cohere_services.yaml
â”‚   â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”‚   â””â”€â”€ test_unit.yaml
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ init.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ ChromaDB/
â”‚   â”‚   â”‚   â”œâ”€â”€ 9c989208-a74c-4934-9e41-5999ad224932/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_level0.bin
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ header.bin
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ length.bin
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ link_list.bin
â”‚   â”‚   â”‚   â”œâ”€â”€ a83eaa7f-fd5b-4339-a258-3254f1a016e4/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ data_level0.bin
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ header.bin
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ length.bin
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ link_list.bin
â”‚   â”‚   â”‚   â””â”€â”€ chroma.sqlite3
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚   â””â”€â”€ test_unit.log
â”‚   â”œâ”€â”€models/                    # Contiene los schemas de la aplicaciÃ³n
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ user_request.py
â”‚   â”œâ”€â”€ Notebooks/
â”‚   â”‚   â”œâ”€â”€ Analisis_token.ipynb
â”‚   â”‚   â”œâ”€â”€ ChromaDB.ipynb
â”‚   â”‚   â”œâ”€â”€ crear_db_con_ChromaDB.ipynb
â”‚   â”‚   â””â”€â”€ crear_db_con_ChromaDB.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cohere_service.py
â”‚   â”‚   â”œâ”€â”€ context_service.py
â”‚   â”‚   â””â”€â”€ openai_service.py.py
â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”œâ”€â”€ e2e/
â”‚   â”‚   â”‚   â””â”€â”€ test_end_to_end.py.py
â”‚   â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”‚   â””â”€â”€ test_integration.py
â”‚   â”‚   â””â”€â”€ unit/
â”‚   â”‚       â”œâ”€â”€ test_unit_api.py
â”‚   â”‚       â””â”€â”€ test_unit_db.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ format_logger.py
â”‚   â”‚   â”œâ”€â”€ funcions_general.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ docs/                    # DocumentaciÃ³n del proyecto
â”‚   â”œâ”€â”€ challenge_ ai_engineer.pdf
â”‚   â”œâ”€â”€ documento.docx
â”‚   â””â”€â”€ tutorial_chroma.ipynb
â”œâ”€â”€ requirements/            # Archivos de requerimientos
â”‚   â”œâ”€â”€ base.txt             # basic requisite dependencies for running the API service.
â”‚   â”œâ”€â”€ dev.txt              # dependencies for the local development.
â”‚   â””â”€â”€ doc.txt              # dependencies for creating sphinx documentation.
â”œâ”€â”€.env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Pi_Challenge_RAG_LLM Collection.postman_collection.json
â””â”€â”€ README.md
```
## Tecnologias

- **Python version**: 3.9 o superior
- **Framework**: FastAPI
- **Database**: ChromaDB
- **LLM**: Cohere y OpenAI
## ConfiguraciÃ³n del Entorno Local

### Clonar el repositorio

```bash
git clone https://github.com/mavixRL/pi_challenge_rag_llm.git
cd pi_challenge_rag_llm
```

### Crear entorno virtual

```bash
# Crear un entorno virtual
python -m venv nombre_entorno_virtual

# Activar entorno virtual (Windows)
.\nombre_entorno_virtual\Scripts\activate

# Activar entorno virtual (macOS/Linux)
source nombre_entorno_virtual/bin/activate
```

### Instalar dependencias

```bash
pip install -r requirements/base.txt
```

### Inicializar aplicaciÃ³n

```bash
uvicorn app.main:app --host localhost --port 8003 --reload
```
La aplicaciÃ³n deberÃ­a estar corriendo en http://127.0.0.1:8003.

Al iniciar la aplicaciÃ³n, se crearÃ¡n dos archivos en la raÃ­z del proyecto: `logger.log`, que contendrÃ¡ los registros del proyecto, y `pi.db`, que serÃ¡ la base de datos de la aplicaciÃ³n.

## DocumentaciÃ³n de la API

Esta secciÃ³n te redirige a Swagger UI, donde puedes interactuar dinÃ¡micamente con tu API y explorar sus endpoints y parÃ¡metros.

- Accede a Swagger UI en `/docs`

La URL para Swagger UI es: http://127.0.0.1:8003/docs (solo de los endpoints de la API)

La documentacion general de todos los modulos se encuentra en la carperta **docs/app/index.html**


##  Mensaje de Bienvenida en la API

  http://localhost:8003/api/

  ```bash
 {
    "mensaje": "Bienvenido a la  API. Desarrollado por Mavix Arias Aguila",
    "message": "Welcome to the API. Developed by Mavix Arias Aguila"
}
  ```
----------
## ImplementaciÃ³n con Docker(Opcional)

### Construir imagen de Docker

```bash
docker build -t pi_challenge .
```

### Inicializar contenedor Docker

```bash
docker run -p 8003:8003 pi_challenge
```

## El Prompt de Cohere

El prompt de Cohere es el siguiente:
El prompt utilizado consiste en ingresar un chat_history con los requerimientos de la respuesta  y el contexto.

El contexto es obtenido a partir del calculo de la similitud de coseno entre los embedding de la pregunta y el contexto.
Luego se obtiene el idioma del query por medio del siguente prompt:
```bash
 chat_history=[
        {
            "role": "SYSTEM",
            "message": f"""Eres un detector muy preciso de idiomas,dime el idioma en una sola palabra (es, en, pt) de la siguiente pregunta : """,  # noqa
        },
    ],
    message=f"""Cual es el idioma del texto?:
{query}""",
...
```
Da como respuesta 
```bash
en es o pt
```
Se omitio el uso de librerias de deteccion de idiomas por las limitaciones de las mismas.

Adicionalmente se tiene un verificador de consulta que es basicamente si la similaridad tiene un valor por debajo de **0.43** se considera que la pregunta no es sobre el documento y se da una respuesta generica (**no_data_msg**).

```bash
no_data_msg:
  es: La pregunta no estÃ¡ relacionada con el documento.
  en: The question is not related to the document.
  pt: A pergunta nÃ£o estÃ¡ relacionada com o documento.

```

con el query, idioma, y contexto se genera el prompt para obtener la respuesta.

```bash
    dict_important1 = {
        "es": "Debes ser lo mÃ¡s conciso y preciso posible en la entrega de informaciÃ³n (20 palabras o menos) y traducir tu respuesta si es necesario a partir del siguiente contexto:",
        "en": "You must be as concise and precise as possible in delivering information (20 words or less) and translate your response if necessary from the following context:",
        "pt": "VocÃª deve ser o mais conciso e preciso possÃ­vel na entrega de informaÃ§Ãµes (20 palavras ou menos) e traduzir sua resposta, se necessÃ¡rio, a partir do seguinte contexto:",
    }

    dict_important2 = {
        "es": "Responde en una sola oraciÃ³n, en el mismo idioma que la pregunta, incluyendo emojis que resuman el contenido de la respuesta, y siempre en tercera persona.",
        "en": "Answer in one sentence, in the same language as the question, including emojis that summarize the content of the answer, and always in third person.",
        "pt": "Responda em uma frase, no mesmo idioma da pergunta, incluindo emojis que resumam o conteÃºdo da resposta, e sempre na terceira pessoa.",
    }
    dict_important3 = {
        "es": "Responde en espaÃ±ol, si el texto esta en espaÃ±ol traduce la respuesta al espaÃ±ol.",
        "en": "Answer in English, if the text is in espaÃ±ol, translate the answer to English.",
        "pt": "Responda em portuguÃªs, se o texto estiver em espaÃ±ol, traduza a resposta para o portuguÃªs.",
    }

    response = co.chat(
        chat_history=[
            {
                "role": "SYSTEM",
                "message": f"""{dict_important1[language]}
            {contexto}
            NOTA IMPORTANTE:
            {dict_important2[language]}

            """,
            },
        ],
        message=f"""{query} {dict_important3[language]}""",

  ```
  por ejemplo si el query es:
  ```bash
  query = "Quien es Zara?"
  ```
  EL prompt seria:
  ```bash
  history_chat:
  """ You must be as concise and precise as possible in delivering information (20 words or less) and translate your response if necessary from the following context:
  FicciÃ³n Espacial: En la lejana galaxia de Zenthoria, dos civilizaciones alienÃ­genas, los Dracorians y los Lumis, se encuentran al borde de la guerra intergalÃ¡ctica. Un intrÃ©pido explorador, Zara, descubre un antiguo artefacto que podrÃ­a contener la clave para la paz. Mientras viaja por planetas hostiles y se enfrenta a desafÃ­os cÃ³smicos, Zara debe desentraÃ±ar los secretos de la reliquia antes de que la galaxia se sumerja en el caos.
  NOTA IMPORTANTE:
  Answer in one sentence, in the same language as the question, including emojis that summarize the content of the answer, and always in third person."""
  mensaje:

  """Quien es Zara? Answer in English, if the text is in espaÃ±ol, translate the answer to English."""

  ```

Y la respuesta seria:

Zara is an intrepid explorer on a mission to unravel the mysteries of an ancient artifact that holds the key to intergalactic peace. ðŸš€",


y por ultmimo se almacena la respuesta, el nombre del usuario y el query en la base de datos para en caso se solicite la misma pregunta buscar en la base de datos y no hacer la consulta al modelo. Evitando en cierta medida el problema de reproducibilidad de los modelos de lenguaje que a pesar de poner temperatura 0 dan respuestas diferentes por su naturaleza probabilistica.


## Pruebas Unitarias

Para ejecutar las pruebas unitarias, ejecute el siguiente comando:

```bash
python -m app.tests.unit.test_unit_api
```
Tener en cuenta que en caso se encuntre algun error hay que revisar el archivo **test_unit_api.py** y el archivo 
**app\config\test\test_unit.yaml**
Donde se define las respuestas que deberian dar los modelos de lenguaje